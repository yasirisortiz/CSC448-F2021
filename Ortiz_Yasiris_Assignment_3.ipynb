{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/yasirisortiz/CSC448-F2021/blob/main/Ortiz_Yasiris_Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports section\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Temperature °C  Mols KCL     Size nm^3\n",
      "0              469       647  6.244743e+05\n",
      "1              403       694  5.779610e+05\n",
      "2              302       975  6.196847e+05\n",
      "3              779       916  1.460449e+06\n",
      "4              901        18  4.325726e+04\n",
      "5              545       637  7.124634e+05\n",
      "6              660       519  7.006960e+05\n",
      "7              143       869  2.718260e+05\n",
      "8               89       461  8.919803e+04\n",
      "9              294       776  4.770210e+05\n",
      "10             991       117  2.441771e+05\n",
      "11             307       781  5.006455e+05\n",
      "12             206        70  3.145200e+04\n",
      "13             437       599  5.390215e+05\n",
      "14             566        75  9.185271e+04\n"
     ]
    }
   ],
   "source": [
    "# Using pandas load the dataset (load remotely, not locally)\n",
    "#df = pd.read_csv(\"science_data_large.csv\")\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/profmcnich/example_notebook/main/science_data_large.csv\")\n",
    "\n",
    "\n",
    "# Output the first 15 rows of the data\n",
    "print(df.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming columns\n",
    "df = df.rename(columns= {\"Temperature °C\":\"Temperature\", \"Mols KCL\":\"Mols\",\"Size nm^3\":\"Size\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table's Summary\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Temperature °C  1000 non-null   int64  \n",
      " 1   Mols KCL        1000 non-null   int64  \n",
      " 2   Size nm^3       1000 non-null   float64\n",
      "dtypes: float64(1), int64(2)\n",
      "memory usage: 23.6 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Display a summary of the table information (number of datapoints, etc.)\n",
    "print(\"Table's Summary\")\n",
    "print()\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the pandas dataset and split it into our features (X) and label (y)\n",
    "# (X) is the two-dimensional array\n",
    "X,y = df.iloc[:,:-1].values, df.iloc[:,-1].values\n",
    "\n",
    "\n",
    "# Use sklearn to split the features and labels into a training/test set. (90% train, 10% test)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=.10)\n",
    "# print(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equation of simple Linear Regression\n",
    "y = c + mX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Perform a Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's Score:  0.8623243730838247 \n",
      "\n",
      "Sample Datapoint with the trained model:  [-418077.92262988] \n",
      "\n",
      "Coefficients from the model:  [ 876.53668988 1048.80128879] \n",
      "\n",
      "Intercept from the model:  -425951.5391434375 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use sklearn to train a model on the training set\n",
    "\n",
    "trained_model = LinearRegression().fit(x_train, y_train)  # fitting the linear model \n",
    "\n",
    "print(\"Model's Score: \",trained_model.score(x_train, y_train), \"\\n\") # score: 0.8668162872421077\n",
    "\n",
    "\n",
    "# Create a sample datapoint and predict the output of that sample with the trained model\n",
    "sample = np.array([[3,5]])\n",
    "print(\"Sample Datapoint with the trained model: \", trained_model.predict(sample), \"\\n\")\n",
    "\n",
    "# Extract the coefficents and intercept from the model and write an equation for your h(x) using LaTeX\n",
    "\n",
    "# coefficients\n",
    "coefficients = trained_model.coef_\n",
    "print(\"Coefficients from the model: \", coefficients, \"\\n\")\n",
    "\n",
    "# intercept \n",
    "intercept  = trained_model.intercept_\n",
    "print(\"Intercept from the model: \", intercept, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Explanation\n",
    "\n",
    "#### Score: 0.8668162872421077\n",
    "The score refers to the predicted score for the trained regression model, it helps to determine the model accurracy and, it is also the way of calculating the coefficient of determination $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equation using the model coefficients\n",
    "\\begin{equation}\n",
    "Equation: \\\\                h(x) = {881.003 * x_0 + 1023.0135 * x_1 -  -415863.416390}\n",
    "\\end{equation}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Use Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the cross_val_score function to repeat your experiment across many shuffles of the data\n",
    "multireg = LinearRegression().fit(x_train, y_train)  # fitting the linear model \n",
    "socre = cross_val_score(multireg, X,y, cv=5) # cv is the number of scores in the array "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report on their finding and their significance\n",
    "Cross Validation allows us to check how is the data performing. We can use CV to shift testing data in multiple parts of the dataset. In this example, I am diving the dataset into 5 parts as testing and the rest of the portion for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5. Using Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score : 1.0 \n",
      "\n",
      "Predict : [4.99694829e+05 1.19722029e+05 4.59800314e+05 2.43109457e+05\n",
      " 7.89596314e+05 2.24347114e+05 3.48355314e+05 1.86374083e+06\n",
      " 7.30240029e+05 7.48334572e+04 1.27018346e+06 7.24028583e+02\n",
      " 2.39418314e+05 4.92891314e+05 4.10720600e+05 1.71829829e+05\n",
      " 9.14373114e+05 7.00945715e+03 8.80350429e+05 3.14520000e+04\n",
      " 7.78384314e+05 1.43166160e+06 2.57202029e+05 3.09613114e+05\n",
      " 6.51534571e+04 6.16366857e+05 3.57346029e+05 1.73457156e+02\n",
      " 3.81815314e+05 8.62471114e+05 5.37476829e+05 3.34174571e+04\n",
      " 1.08429600e+05 1.10090883e+06 1.39883700e+06 1.79065714e+05\n",
      " 6.71197457e+05 7.15664114e+05 1.24615340e+06 9.02100600e+05\n",
      " 4.78631429e+03 4.32728286e+04 5.09602859e+03 3.04432457e+05\n",
      " 4.46968286e+04 8.64352114e+05 5.56986600e+05 1.70568429e+05\n",
      " 2.00457257e+05 8.94889029e+05 3.89294314e+05 5.02017029e+05\n",
      " 3.60092314e+05 7.00696029e+05 1.65682572e+04 7.38660286e+04\n",
      " 5.54582859e+03 1.19853829e+05 9.32680286e+04 2.11805600e+05\n",
      " 8.73038314e+05 2.00866257e+05 1.10457257e+05 3.18040000e+04\n",
      " 1.01681643e+06 7.95132257e+05 9.18527143e+04 4.62369857e+05\n",
      " 1.09724626e+06 7.32261029e+05 2.90225715e+03 7.04410857e+05\n",
      " 2.50464114e+05 7.36484029e+05 4.42691143e+04 5.21373600e+05\n",
      " 7.31992257e+05 5.47971429e+05 6.34284314e+05 1.81536343e+06\n",
      " 4.26357257e+05 8.11563143e+04 7.18007429e+05 2.41597829e+05\n",
      " 1.44585886e+06 1.15613257e+05 1.70761660e+06 8.08596257e+05\n",
      " 7.11794571e+04 1.66231457e+05 1.03501829e+05 7.04621829e+05\n",
      " 6.64790600e+05 5.77260314e+05 7.58047114e+05 6.24474257e+05\n",
      " 3.68860286e+04 1.07111430e+03 4.04337143e+04 7.78494571e+04] \n",
      "\n",
      "Coefficient: [ 0.00000000e+00  1.20000000e+01 -1.11045665e-07 -1.96642702e-11\n",
      "  2.00000000e+00  2.85714287e-02] \n",
      "\n",
      "Intercept: 1.4065648429095745e-05 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using the PolynomialFeatures library perform another regression on an augmented dataset of degree 2\n",
    "poly = PolynomialFeatures(degree = 2)\n",
    "\n",
    "x_train = poly.fit_transform(x_train)\n",
    "x_test = poly.fit_transform(x_test)\n",
    "\n",
    "model = BayesianRidge()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# model\n",
    "print(f\"Score : {model.score(x_train,y_train)}\", \"\\n\")                     \n",
    "print(f\"Predict : {model.predict(x_test)}\", \"\\n\")\n",
    "\n",
    "# # Report on the metrics and output the resultant equation as you did in Part 3.\n",
    "   \n",
    "print(f\"Coefficient: {model.coef_}\", \"\\n\")\n",
    "print(f\"Intercept: {model.intercept_}\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([34.17144233])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array([[1,2,3,4,5,6]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "Equation: \\\\                y = {2.33 * 10^(-5) + 0.000000000 * 1 + 1.20000000 * 10^1 a + -1.76090 * 10^7 * b+ -2.64489* 10^11 * (a^2)+ 2.000000 * (ab)+ 2.8571 * 10^-2 * (b^2)}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
